#!/usr/bin/env python3
"""
é«˜çº§éŸ³é¢‘åˆ†æå·¥å…·
æä¾›æ›´å¤šéŸ³é¢‘ç‰¹å¾åˆ†æåŠŸèƒ½ï¼Œç”¨äºæ™ºèƒ½è§†é¢‘å‰ªè¾‘
"""

import librosa
import numpy as np
import matplotlib.pyplot as plt
import json
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from audio_beat_detection import AudioBeatDetector


@dataclass
class AudioSegment:
    """éŸ³é¢‘ç‰‡æ®µæ•°æ®ç±»"""
    start_time: float
    end_time: float
    energy_level: float
    spectral_centroid: float
    tempo_stability: float
    segment_type: str  # 'intro', 'verse', 'chorus', 'bridge', 'outro'


class AdvancedAudioAnalyzer(AudioBeatDetector):
    """é«˜çº§éŸ³é¢‘åˆ†æå™¨ï¼Œç»§æ‰¿åŸºç¡€èŠ‚æ‹æ£€æµ‹åŠŸèƒ½"""
    
    def __init__(self, sample_rate: int = 22050):
        super().__init__(sample_rate)
        self.segments = []
        self.energy_profile = None
        self.spectral_features = {}
        
    def analyze_energy_profile(self, frame_length: int = 2048, hop_length: int = 512) -> np.ndarray:
        """
        åˆ†æéŸ³é¢‘èƒ½é‡åˆ†å¸ƒ
        
        Args:
            frame_length: å¸§é•¿åº¦
            hop_length: è·³è·ƒé•¿åº¦
            
        Returns:
            np.ndarray: èƒ½é‡åˆ†å¸ƒæ•°ç»„
        """
        if self.audio_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½éŸ³é¢‘æ–‡ä»¶")
        
        # è®¡ç®—çŸ­æ—¶èƒ½é‡
        energy = []
        for i in range(0, len(self.audio_data) - frame_length, hop_length):
            frame = self.audio_data[i:i + frame_length]
            frame_energy = np.sum(frame ** 2)
            energy.append(frame_energy)
        
        self.energy_profile = np.array(energy)
        return self.energy_profile
    
    def detect_music_structure(self) -> List[AudioSegment]:
        """
        æ£€æµ‹éŸ³ä¹ç»“æ„ï¼ˆæ®µè½åˆ†æï¼‰
        
        Returns:
            List[AudioSegment]: éŸ³é¢‘æ®µè½åˆ—è¡¨
        """
        if self.audio_data is None:
            raise ValueError("è¯·å…ˆåŠ è½½éŸ³é¢‘æ–‡ä»¶")
        
        print("æ­£åœ¨åˆ†æéŸ³ä¹ç»“æ„...")
        
        # è®¡ç®—è‰²åº¦ç‰¹å¾ï¼ˆç”¨äºå’Œå£°åˆ†æï¼‰
        chroma = librosa.feature.chroma_stft(
            y=self.audio_data, 
            sr=self.sample_rate,
            hop_length=512
        )
        
        # è®¡ç®—MFCCç‰¹å¾
        mfcc = librosa.feature.mfcc(
            y=self.audio_data, 
            sr=self.sample_rate,
            n_mfcc=13,
            hop_length=512
        )
        
        # è®¡ç®—é¢‘è°±è´¨å¿ƒ
        spectral_centroids = librosa.feature.spectral_centroid(
            y=self.audio_data, 
            sr=self.sample_rate,
            hop_length=512
        )[0]
        
        # åˆ†æèƒ½é‡åˆ†å¸ƒ
        if self.energy_profile is None:
            self.analyze_energy_profile()
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ†ææ®µè½
        window_size = int(self.sample_rate * 4)  # 4ç§’çª—å£
        hop_size = int(self.sample_rate * 2)     # 2ç§’è·³è·ƒ
        
        segments = []
        audio_duration = len(self.audio_data) / self.sample_rate
        
        for start_sample in range(0, len(self.audio_data) - window_size, hop_size):
            end_sample = min(start_sample + window_size, len(self.audio_data))
            
            start_time = start_sample / self.sample_rate
            end_time = end_sample / self.sample_rate
            
            # è®¡ç®—è¯¥æ®µè½çš„ç‰¹å¾
            segment_audio = self.audio_data[start_sample:end_sample]
            
            # èƒ½é‡æ°´å¹³
            energy_level = np.mean(segment_audio ** 2)
            
            # é¢‘è°±è´¨å¿ƒï¼ˆéŸ³è‰²äº®åº¦ï¼‰
            start_frame = start_sample // 512
            end_frame = end_sample // 512
            segment_centroid = np.mean(spectral_centroids[start_frame:end_frame])
            
            # èŠ‚æ‹ç¨³å®šæ€§ï¼ˆå¦‚æœæœ‰èŠ‚æ‹æ•°æ®ï¼‰
            tempo_stability = 1.0
            if self.beat_times is not None:
                segment_beats = self.beat_times[
                    (self.beat_times >= start_time) & (self.beat_times <= end_time)
                ]
                if len(segment_beats) > 2:
                    beat_intervals = np.diff(segment_beats)
                    tempo_stability = 1.0 / (1.0 + np.std(beat_intervals))
            
            # ç®€å•çš„æ®µè½ç±»å‹åˆ†ç±»
            segment_type = self._classify_segment(
                start_time, end_time, audio_duration, 
                energy_level, segment_centroid
            )
            
            segment = AudioSegment(
                start_time=start_time,
                end_time=end_time,
                energy_level=energy_level,
                spectral_centroid=segment_centroid,
                tempo_stability=tempo_stability,
                segment_type=segment_type
            )
            
            segments.append(segment)
        
        self.segments = segments
        print(f"æ£€æµ‹åˆ° {len(segments)} ä¸ªéŸ³é¢‘æ®µè½")
        return segments
    
    def _classify_segment(self, start_time: float, end_time: float, 
                         total_duration: float, energy: float, 
                         centroid: float) -> str:
        """
        ç®€å•çš„æ®µè½åˆ†ç±»é€»è¾‘
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            total_duration: æ€»æ—¶é•¿
            energy: èƒ½é‡æ°´å¹³
            centroid: é¢‘è°±è´¨å¿ƒ
            
        Returns:
            str: æ®µè½ç±»å‹
        """
        # åŸºäºä½ç½®çš„åˆæ­¥åˆ†ç±»
        position_ratio = start_time / total_duration
        
        if position_ratio < 0.1:
            return 'intro'
        elif position_ratio > 0.9:
            return 'outro'
        elif energy > np.percentile([s.energy_level for s in self.segments if s.energy_level], 75):
            return 'chorus'  # é«˜èƒ½é‡æ®µè½é€šå¸¸æ˜¯å‰¯æ­Œ
        elif centroid < np.percentile([s.spectral_centroid for s in self.segments if s.spectral_centroid], 25):
            return 'bridge'  # ä½é¢‘è°±è´¨å¿ƒå¯èƒ½æ˜¯è¿‡æ¸¡æ®µ
        else:
            return 'verse'   # é»˜è®¤ä¸ºä¸»æ­Œ
    
    def find_highlight_moments(self, top_n: int = 5) -> List[Tuple[float, float, str]]:
        """
        æ‰¾åˆ°éŸ³é¢‘ä¸­çš„é«˜å…‰æ—¶åˆ»ï¼ˆé€‚åˆåšå¡ç‚¹çš„ä½ç½®ï¼‰
        
        Args:
            top_n: è¿”å›å‰Nä¸ªé«˜å…‰æ—¶åˆ»
            
        Returns:
            List[Tuple[float, float, str]]: (æ—¶é—´, å¼ºåº¦, æè¿°)
        """
        if not self.segments:
            self.detect_music_structure()
        
        highlights = []
        
        # åŸºäºèƒ½é‡å˜åŒ–æ‰¾é«˜å…‰æ—¶åˆ»
        for i, segment in enumerate(self.segments):
            if i == 0:
                continue
                
            prev_segment = self.segments[i-1]
            energy_change = segment.energy_level - prev_segment.energy_level
            
            # èƒ½é‡çªç„¶å¢åŠ çš„æ—¶åˆ»
            if energy_change > 0:
                intensity = energy_change * segment.tempo_stability
                description = f"èƒ½é‡çˆ†å‘ ({segment.segment_type})"
                highlights.append((segment.start_time, intensity, description))
        
        # åŸºäºèŠ‚æ‹æ‰¾å¼ºæ‹ä½ç½®
        if self.beat_times is not None:
            for i, beat_time in enumerate(self.beat_times):
                if i % 4 == 0:  # æ¯4æ‹çš„å¼ºæ‹
                    # æ‰¾åˆ°å¯¹åº”çš„æ®µè½
                    segment = next((s for s in self.segments 
                                  if s.start_time <= beat_time <= s.end_time), None)
                    if segment:
                        intensity = segment.energy_level * 0.8
                        description = f"å¼ºæ‹ä½ç½® ({segment.segment_type})"
                        highlights.append((beat_time, intensity, description))
        
        # æŒ‰å¼ºåº¦æ’åºå¹¶è¿”å›å‰Nä¸ª
        highlights.sort(key=lambda x: x[1], reverse=True)
        return highlights[:top_n]
    
    def generate_cut_points(self, target_duration: float = 60.0, 
                           style: str = 'dynamic') -> List[Tuple[float, float]]:
        """
        ç”Ÿæˆæ™ºèƒ½å‰ªè¾‘ç‚¹å»ºè®®
        
        Args:
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰
            style: å‰ªè¾‘é£æ ¼ ('dynamic', 'smooth', 'rhythmic')
            
        Returns:
            List[Tuple[float, float]]: (å¼€å§‹æ—¶é—´, ç»“æŸæ—¶é—´) åˆ—è¡¨
        """
        if not self.segments:
            self.detect_music_structure()
        
        cut_points = []
        total_audio_duration = len(self.audio_data) / self.sample_rate
        
        if style == 'dynamic':
            # åŠ¨æ€å‰ªè¾‘ï¼šé€‰æ‹©é«˜èƒ½é‡æ®µè½
            high_energy_segments = sorted(
                self.segments, 
                key=lambda s: s.energy_level, 
                reverse=True
            )
            
            current_duration = 0
            for segment in high_energy_segments:
                if current_duration >= target_duration:
                    break
                
                segment_duration = segment.end_time - segment.start_time
                if current_duration + segment_duration <= target_duration:
                    cut_points.append((segment.start_time, segment.end_time))
                    current_duration += segment_duration
                else:
                    # éƒ¨åˆ†ä½¿ç”¨è¯¥æ®µè½
                    remaining_time = target_duration - current_duration
                    cut_points.append((segment.start_time, segment.start_time + remaining_time))
                    break
        
        elif style == 'smooth':
            # å¹³æ»‘å‰ªè¾‘ï¼šæŒ‰æ—¶é—´é¡ºåºé€‰æ‹©
            segment_duration = target_duration / len(self.segments)
            for segment in self.segments:
                if len(cut_points) * segment_duration >= target_duration:
                    break
                
                end_time = min(segment.start_time + segment_duration, segment.end_time)
                cut_points.append((segment.start_time, end_time))
        
        elif style == 'rhythmic':
            # èŠ‚å¥å‰ªè¾‘ï¼šåŸºäºèŠ‚æ‹ç‚¹
            if self.beat_times is not None:
                beats_per_cut = max(1, len(self.beat_times) // int(target_duration / 4))
                
                for i in range(0, len(self.beat_times), beats_per_cut):
                    if i + beats_per_cut < len(self.beat_times):
                        start_time = self.beat_times[i]
                        end_time = self.beat_times[i + beats_per_cut]
                        
                        if end_time - start_time <= target_duration:
                            cut_points.append((start_time, end_time))
                        else:
                            break
        
        return cut_points
    
    def export_analysis_report(self, output_path: str) -> bool:
        """
        å¯¼å‡ºå®Œæ•´çš„åˆ†ææŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: å¯¼å‡ºæ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–åŸºç¡€åˆ†ææ•°æ®
            rhythm_analysis = self.analyze_rhythm_patterns()
            
            # è·å–é«˜å…‰æ—¶åˆ»
            highlights = self.find_highlight_moments()
            
            # ç”Ÿæˆå‰ªè¾‘å»ºè®®
            dynamic_cuts = self.generate_cut_points(60.0, 'dynamic')
            smooth_cuts = self.generate_cut_points(60.0, 'smooth')
            rhythmic_cuts = self.generate_cut_points(60.0, 'rhythmic')
            
            report = {
                'metadata': {
                    'audio_duration': rhythm_analysis['audio_duration'],
                    'tempo_bpm': rhythm_analysis['tempo_bpm'],
                    'total_segments': len(self.segments),
                    'analysis_timestamp': str(np.datetime64('now'))
                },
                'rhythm_analysis': rhythm_analysis,
                'music_structure': [
                    {
                        'start_time': seg.start_time,
                        'end_time': seg.end_time,
                        'duration': seg.end_time - seg.start_time,
                        'energy_level': seg.energy_level,
                        'spectral_centroid': seg.spectral_centroid,
                        'tempo_stability': seg.tempo_stability,
                        'segment_type': seg.segment_type
                    }
                    for seg in self.segments
                ],
                'highlight_moments': [
                    {
                        'time': time,
                        'intensity': intensity,
                        'description': desc
                    }
                    for time, intensity, desc in highlights
                ],
                'cut_suggestions': {
                    'dynamic_style': [
                        {'start': start, 'end': end, 'duration': end - start}
                        for start, end in dynamic_cuts
                    ],
                    'smooth_style': [
                        {'start': start, 'end': end, 'duration': end - start}
                        for start, end in smooth_cuts
                    ],
                    'rhythmic_style': [
                        {'start': start, 'end': end, 'duration': end - start}
                        for start, end in rhythmic_cuts
                    ]
                }
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print(f"å®Œæ•´åˆ†ææŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_path}")
            return True
            
        except Exception as e:
            print(f"å¯¼å‡ºåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def visualize_complete_analysis(self, output_path: Optional[str] = None):
        """
        ç”Ÿæˆå®Œæ•´çš„åˆ†æå¯è§†åŒ–å›¾è¡¨
        
        Args:
            output_path: å›¾ç‰‡ä¿å­˜è·¯å¾„
        """
        if self.audio_data is None:
            print("è¯·å…ˆåŠ è½½éŸ³é¢‘æ–‡ä»¶")
            return
        
        # ç¡®ä¿æœ‰æ‰€æœ‰åˆ†ææ•°æ®
        if not self.segments:
            self.detect_music_structure()
        
        if self.energy_profile is None:
            self.analyze_energy_profile()
        
        # åˆ›å»ºæ—¶é—´è½´
        time_axis = np.linspace(0, len(self.audio_data) / self.sample_rate, len(self.audio_data))
        energy_time_axis = np.linspace(0, len(self.audio_data) / self.sample_rate, len(self.energy_profile))
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(4, 1, figsize=(16, 12))
        
        # å­å›¾1: éŸ³é¢‘æ³¢å½¢ä¸æ®µè½æ ‡è®°
        axes[0].plot(time_axis, self.audio_data, alpha=0.6, color='blue')
        for segment in self.segments:
            color = {'intro': 'green', 'verse': 'blue', 'chorus': 'red', 
                    'bridge': 'orange', 'outro': 'purple'}.get(segment.segment_type, 'gray')
            axes[0].axvspan(segment.start_time, segment.end_time, alpha=0.3, color=color)
        axes[0].set_title('éŸ³é¢‘æ³¢å½¢ä¸éŸ³ä¹ç»“æ„åˆ†æ', fontsize=14)
        axes[0].set_ylabel('æŒ¯å¹…')
        axes[0].grid(True, alpha=0.3)
        
        # å­å›¾2: èƒ½é‡åˆ†å¸ƒ
        axes[1].plot(energy_time_axis, self.energy_profile, color='red', linewidth=2)
        axes[1].set_title('éŸ³é¢‘èƒ½é‡åˆ†å¸ƒ', fontsize=14)
        axes[1].set_ylabel('èƒ½é‡')
        axes[1].grid(True, alpha=0.3)
        
        # å­å›¾3: èŠ‚æ‹ç‚¹ä¸é«˜å…‰æ—¶åˆ»
        if self.beat_times is not None:
            axes[2].vlines(self.beat_times, 0, 1, colors='blue', alpha=0.6, label='èŠ‚æ‹ç‚¹')
        
        highlights = self.find_highlight_moments(10)
        highlight_times = [h[0] for h in highlights]
        highlight_intensities = [h[1] for h in highlights]
        axes[2].scatter(highlight_times, [0.8] * len(highlight_times), 
                       c='red', s=100, alpha=0.8, label='é«˜å…‰æ—¶åˆ»')
        axes[2].set_title('èŠ‚æ‹ç‚¹ä¸é«˜å…‰æ—¶åˆ»', fontsize=14)
        axes[2].set_ylabel('æ ‡è®°')
        axes[2].set_ylim(0, 1)
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)
        
        # å­å›¾4: æ®µè½ç±»å‹åˆ†å¸ƒ
        segment_types = [seg.segment_type for seg in self.segments]
        segment_starts = [seg.start_time for seg in self.segments]
        segment_energies = [seg.energy_level for seg in self.segments]
        
        colors = {'intro': 'green', 'verse': 'blue', 'chorus': 'red', 
                 'bridge': 'orange', 'outro': 'purple'}
        for i, (start, seg_type, energy) in enumerate(zip(segment_starts, segment_types, segment_energies)):
            axes[3].bar(start, energy, width=2.0, alpha=0.7, 
                       color=colors.get(seg_type, 'gray'), label=seg_type if i == 0 else "")
        
        axes[3].set_title('æ®µè½ç±»å‹ä¸èƒ½é‡åˆ†å¸ƒ', fontsize=14)
        axes[3].set_ylabel('èƒ½é‡æ°´å¹³')
        axes[3].set_xlabel('æ—¶é—´ (ç§’)')
        axes[3].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"å®Œæ•´åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
        
        plt.show()


def main():
    """ç¤ºä¾‹ä½¿ç”¨"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python advanced_audio_analyzer.py <éŸ³é¢‘æ–‡ä»¶>")
        return
    
    audio_file = sys.argv[1]
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = AdvancedAudioAnalyzer()
    
    # åŠ è½½éŸ³é¢‘
    if not analyzer.load_audio(audio_file):
        return
    
    # æ‰§è¡Œå®Œæ•´åˆ†æ
    print("æ‰§è¡Œå®Œæ•´éŸ³é¢‘åˆ†æ...")
    analyzer.detect_tempo_and_beats()
    analyzer.detect_music_structure()
    
    # å¯¼å‡ºåˆ†ææŠ¥å‘Š
    report_file = f"{Path(audio_file).stem}_analysis_report.json"
    analyzer.export_analysis_report(report_file)
    
    # ç”Ÿæˆå¯è§†åŒ–
    viz_file = f"{Path(audio_file).stem}_complete_analysis.png"
    analyzer.visualize_complete_analysis(viz_file)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š åˆ†ææŠ¥å‘Š: {report_file}")
    print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {viz_file}")


if __name__ == "__main__":
    main()
