#!/usr/bin/env python3
"""
Geminiè§†é¢‘åˆ†æå™¨
ä½¿ç”¨Google Gemini APIåˆ†æè§†é¢‘å†…å®¹ï¼Œå‚è€ƒ @mixvideo/gemini åŒ…çš„å®ç°æ¨¡å¼
"""

import os
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Callable
import requests
import logging
from dataclasses import dataclass, asdict


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class GeminiConfig:
    """Geminié…ç½®ç±»"""
    # è®¤è¯é…ç½®
    cloudflare_project_id: str = ""
    cloudflare_gateway_id: str = ""
    google_project_id: str = ""
    regions: List[str] = None
    access_token: str = ""

    # APIé…ç½®
    model_name: str = "gemini-2.5-flash"
    base_url: str = "https://bowongai-dev--bowong-ai-video-gemini-fastapi-webapp.modal.run"
    bearer_token: str = "bowong7777"
    timeout: int = 120

    # ç¼“å­˜é…ç½®
    enable_cache: bool = True
    cache_dir: str = ".cache/gemini_analysis"
    cache_expiry: int = 7 * 24 * 3600  # 7å¤©

    # é‡è¯•é…ç½®
    max_retries: int = 3
    retry_delay: int = 5

    def __post_init__(self):
        if self.regions is None:
            self.regions = ["us-central1", "us-east1", "europe-west1"]


@dataclass
class AnalysisProgress:
    """åˆ†æè¿›åº¦"""
    step: str
    progress: int  # 0-100
    description: str = ""
    current_file: str = ""
    stage: str = "upload"  # upload, analysis, complete


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    video_path: str
    file_uri: str
    prompt: str
    result: Dict[str, Any]
    timestamp: float
    checksum: str
    model_name: str


class GeminiVideoAnalyzer:
    """Geminiè§†é¢‘åˆ†æå™¨ - å‚è€ƒ @mixvideo/gemini å®ç°"""

    def __init__(self, config: Optional[GeminiConfig] = None):
        """
        åˆå§‹åŒ–Geminiè§†é¢‘åˆ†æå™¨

        Args:
            config: Geminié…ç½®å¯¹è±¡
        """
        self.config = config or GeminiConfig()
        self._access_token = None
        self._token_expires_at = None

        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if self.config.enable_cache:
            os.makedirs(self.config.cache_dir, exist_ok=True)

    async def get_access_token(self) -> str:
        """
        è·å–Googleè®¿é—®ä»¤ç‰Œï¼Œå‚è€ƒ useGeminiAccessToken å®ç°
        """
        # æ£€æŸ¥ç¼“å­˜çš„ä»¤ç‰Œæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        if (self._access_token and self._token_expires_at and
            time.time() < self._token_expires_at - 300):  # æå‰5åˆ†é’Ÿåˆ·æ–°
            return self._access_token

        try:
            headers = {
                "Authorization": f"Bearer {self.config.bearer_token}",
                "Content-Type": "application/json"
            }

            response = requests.get(
                f"{self.config.base_url}/google/access-token",
                headers=headers,
                timeout=self.config.timeout
            )

            if response.status_code != 200:
                raise Exception(f"è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {response.status_code} - {response.text}")

            token_data = response.json()
            self._access_token = token_data["access_token"]
            self._token_expires_at = time.time() + token_data.get("expires_in", 3600)

            logger.info("âœ… æˆåŠŸè·å–Googleè®¿é—®ä»¤ç‰Œ")
            return self._access_token

        except Exception as e:
            logger.error(f"âŒ è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {e}")
            raise Exception(f"è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {str(e)}")

    def _create_gemini_client(self, access_token: str) -> Dict[str, Any]:
        """
        åˆ›å»ºGeminiå®¢æˆ·ç«¯é…ç½®ï¼Œå‚è€ƒ GoogleGenaiClient å®ç°
        """
        import random

        # éšæœºé€‰æ‹©åŒºåŸŸ
        region = random.choice(self.config.regions)

        gateway_url = (
            f"https://gateway.ai.cloudflare.com/v1/"
            f"{self.config.cloudflare_project_id}/"
            f"{self.config.cloudflare_gateway_id}/"
            f"google-vertex-ai/v1/projects/"
            f"{self.config.google_project_id}/"
            f"locations/{region}/publishers/google/models"
        )

        return {
            "gateway_url": gateway_url,
            "access_token": access_token,
            "headers": {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {access_token}"
            }
        }

    def _calculate_file_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        import hashlib

        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _generate_cache_key(self, video_path: str, prompt: str, model_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib

        key_data = f"{video_path}:{prompt}:{model_name}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _check_analysis_cache(self, video_path: str, prompt: str) -> Optional[Dict[str, Any]]:
        """æ£€æŸ¥åˆ†æç¼“å­˜"""
        if not self.config.enable_cache:
            return None

        try:
            cache_key = self._generate_cache_key(video_path, prompt, self.config.model_name)
            cache_file = os.path.join(self.config.cache_dir, f"{cache_key}.json")

            if not os.path.exists(cache_file):
                return None

            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_entry_data = json.load(f)
                cache_entry = CacheEntry(**cache_entry_data)

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if time.time() - cache_entry.timestamp > self.config.cache_expiry:
                os.unlink(cache_file)
                logger.info(f"â° ç¼“å­˜å·²è¿‡æœŸ: {Path(video_path).name}")
                return None

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å‘ç”Ÿå˜åŒ–
            current_checksum = self._calculate_file_checksum(video_path)
            if current_checksum != cache_entry.checksum:
                os.unlink(cache_file)
                logger.info(f"ğŸ”„ æ–‡ä»¶å·²å˜æ›´: {Path(video_path).name}")
                return None

            logger.info(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ: {Path(video_path).name}")
            return cache_entry.result

        except Exception as e:
            logger.warning(f"æ£€æŸ¥åˆ†æç¼“å­˜å¤±è´¥: {e}")
            return None

    def _save_analysis_cache(self, video_path: str, file_uri: str, prompt: str, result: Dict[str, Any]) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜"""
        if not self.config.enable_cache:
            return

        try:
            cache_key = self._generate_cache_key(video_path, prompt, self.config.model_name)
            cache_file = os.path.join(self.config.cache_dir, f"{cache_key}.json")

            checksum = self._calculate_file_checksum(video_path)
            cache_entry = CacheEntry(
                video_path=video_path,
                file_uri=file_uri,
                prompt=prompt,
                result=result,
                timestamp=time.time(),
                checksum=checksum,
                model_name=self.config.model_name
            )

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(cache_entry), f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ç¼“å­˜: {Path(video_path).name}")

        except Exception as e:
            logger.warning(f"ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {e}")

    def analyze_video(self, video_path: str, prompt: str, progress_callback: Optional[Callable[[AnalysisProgress], None]] = None) -> Dict[str, Any]:
        """
        åˆ†æè§†é¢‘å†…å®¹

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            prompt: åˆ†ææç¤ºè¯
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            progress = AnalysisProgress(
                step="å¼€å§‹åˆ†æ",
                progress=0,
                current_file=Path(video_path).name,
                stage="upload"
            )
            if progress_callback:
                progress_callback(progress)

            # æ£€æŸ¥ç¼“å­˜
            progress.step = "æ£€æŸ¥ç¼“å­˜"
            progress.progress = 5
            if progress_callback:
                progress_callback(progress)

            cached_result = self._check_analysis_cache(video_path, prompt)
            if cached_result:
                progress.step = "ä½¿ç”¨ç¼“å­˜ç»“æœ"
                progress.progress = 100
                progress.stage = "complete"
                if progress_callback:
                    progress_callback(progress)
                return cached_result

            # ä¸Šä¼ è§†é¢‘æ–‡ä»¶
            progress.step = "ä¸Šä¼ è§†é¢‘æ–‡ä»¶"
            progress.progress = 10
            if progress_callback:
                progress_callback(progress)

            file_uri = self._upload_video_file(video_path, progress_callback)

            # å‘é€åˆ†æè¯·æ±‚
            progress.step = "å‘é€åˆ†æè¯·æ±‚"
            progress.progress = 60
            progress.stage = "analysis"
            if progress_callback:
                progress_callback(progress)

            result = self._generate_content(file_uri, prompt, progress_callback)

            # è§£æç»“æœ
            progress.step = "å¤„ç†åˆ†æç»“æœ"
            progress.progress = 90
            if progress_callback:
                progress_callback(progress)

            parsed_result = self._parse_analysis_result(result, video_path)

            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_analysis_cache(video_path, file_uri, prompt, parsed_result)

            progress.step = "åˆ†æå®Œæˆ"
            progress.progress = 100
            progress.stage = "complete"
            if progress_callback:
                progress_callback(progress)

            return parsed_result

        except Exception as e:
            logger.error(f"âŒ è§†é¢‘åˆ†æå¤±è´¥: {e}")
            raise Exception(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")

    def _upload_video_file(self, video_path: str, progress_callback: Optional[Callable[[AnalysisProgress], None]] = None) -> str:
        """
        ä¸Šä¼ è§†é¢‘æ–‡ä»¶åˆ°Geminiï¼Œå‚è€ƒ uploadFileToGemini å®ç°

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            æ–‡ä»¶URI
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(video_path)
            max_size = 100 * 1024 * 1024  # 100MBé™åˆ¶

            if file_size > max_size:
                raise Exception(f"è§†é¢‘æ–‡ä»¶è¿‡å¤§ ({file_size / 1024 / 1024:.1f}MB)ï¼Œè¯·ä½¿ç”¨å°äº100MBçš„æ–‡ä»¶")

            # è·å–è®¿é—®ä»¤ç‰Œ
            import asyncio
            access_token = asyncio.run(self.get_access_token())

            progress = AnalysisProgress(
                step="å‡†å¤‡ä¸Šä¼ æ•°æ®",
                progress=20,
                current_file=Path(video_path).name,
                stage="upload"
            )
            if progress_callback:
                progress_callback(progress)

            # å‡†å¤‡FormData
            with open(video_path, 'rb') as f:
                video_data = f.read()

            # ä½¿ç”¨æ–°çš„ä¸Šä¼ API
            files = {
                'file': (Path(video_path).name, video_data, 'video/mp4')
            }

            headers = {
                "Authorization": f"Bearer {access_token}",
                "x-google-api-key": access_token,
            }

            progress.step = "ä¸Šä¼ åˆ°Vertex AI"
            progress.progress = 30
            if progress_callback:
                progress_callback(progress)

            # ä¸Šä¼ åˆ°Vertex AI
            upload_url = f"{self.config.base_url}/google/vertex-ai/upload"
            params = {
                "bucket": "dy-media-storage",
                "prefix": "video-analysis"
            }

            response = requests.post(
                upload_url,
                files=files,
                headers=headers,
                params=params,
                timeout=self.config.timeout
            )

            if response.status_code != 200:
                raise Exception(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.status_code} - {response.text}")

            upload_result = response.json()
            file_uri = upload_result.get('urn') or upload_result.get('uri')

            if not file_uri:
                raise Exception("ä¸Šä¼ æˆåŠŸä½†æœªè·å–åˆ°æ–‡ä»¶URI")

            progress.step = "ä¸Šä¼ å®Œæˆ"
            progress.progress = 50
            if progress_callback:
                progress_callback(progress)

            logger.info(f"âœ… è§†é¢‘ä¸Šä¼ æˆåŠŸ: {Path(video_path).name} -> {file_uri}")
            return file_uri

        except Exception as e:
            logger.error(f"âŒ è§†é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            raise Exception(f"è§†é¢‘ä¸Šä¼ å¤±è´¥: {str(e)}")

    def _generate_content(self, file_uri: str, prompt: str, progress_callback: Optional[Callable[[AnalysisProgress], None]] = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆå†…å®¹åˆ†æï¼Œå‚è€ƒ GoogleGenaiClient.generateContent å®ç°

        Args:
            file_uri: è§†é¢‘æ–‡ä»¶URI
            prompt: åˆ†ææç¤ºè¯
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            APIå“åº”ç»“æœ
        """
        try:
            # è·å–è®¿é—®ä»¤ç‰Œ
            import asyncio
            access_token = asyncio.run(self.get_access_token())

            # åˆ›å»ºå®¢æˆ·ç«¯é…ç½®
            client_config = self._create_gemini_client(access_token)

            # æ ¼å¼åŒ–GCS URI
            formatted_uri = self._format_gcs_uri(file_uri)

            # å‡†å¤‡è¯·æ±‚æ•°æ®ï¼Œå‚è€ƒ TypeScript å®ç°
            request_data = {
                "contents": [
                    {
                        "role": "user",
                        "parts": [
                            {
                                "text": prompt
                            },
                            {
                                "fileData": {
                                    "mimeType": "video/mp4",
                                    "fileUri": formatted_uri
                                }
                            }
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.1,
                    "topK": 32,
                    "topP": 1,
                    "maxOutputTokens": 8192
                }
            }

            progress = AnalysisProgress(
                step="ç­‰å¾…AIåˆ†æ",
                progress=70,
                current_file="",
                stage="analysis"
            )
            if progress_callback:
                progress_callback(progress)

            # å‘é€è¯·æ±‚åˆ°Cloudflare Gateway
            generate_url = f"{client_config['gateway_url']}/{self.config.model_name}:generateContent"

            logger.info(f"ğŸ“¤ å‘é€ Gemini API è¯·æ±‚: {formatted_uri}")

            # é‡è¯•æœºåˆ¶
            last_exception = None
            for attempt in range(self.config.max_retries):
                try:
                    response = requests.post(
                        generate_url,
                        headers=client_config["headers"],
                        json=request_data,
                        timeout=self.config.timeout
                    )

                    if response.status_code == 200:
                        result = response.json()

                        if 'candidates' not in result or not result['candidates']:
                            raise Exception("APIè¿”å›ç»“æœä¸ºç©º")

                        logger.info("âœ… æˆåŠŸè·å–Geminiåˆ†æç»“æœ")
                        return result
                    else:
                        error_msg = f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                        logger.warning(f"âš ï¸ å°è¯• {attempt + 1}/{self.config.max_retries}: {error_msg}")

                        if attempt == self.config.max_retries - 1:
                            raise Exception(error_msg)

                        time.sleep(self.config.retry_delay)

                except requests.exceptions.Timeout as e:
                    last_exception = e
                    logger.warning(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œå°è¯• {attempt + 1}/{self.config.max_retries}")
                    if attempt < self.config.max_retries - 1:
                        time.sleep(self.config.retry_delay)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œå°è¯• {attempt + 1}/{self.config.max_retries}: {e}")
                    if attempt < self.config.max_retries - 1:
                        time.sleep(self.config.retry_delay)

            raise Exception(f"å†…å®¹ç”Ÿæˆå¤±è´¥ï¼Œå·²é‡è¯•{self.config.max_retries}æ¬¡: {last_exception}")

        except Exception as e:
            logger.error(f"âŒ å†…å®¹ç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")

    def _format_gcs_uri(self, file_uri: str) -> str:
        """æ ¼å¼åŒ–GCS URI"""
        if file_uri.startswith('gs://'):
            return file_uri
        elif file_uri.startswith('https://storage.googleapis.com/'):
            # è½¬æ¢ä¸ºgs://æ ¼å¼
            path = file_uri.replace('https://storage.googleapis.com/', '')
            return f"gs://{path}"
        else:
            # å‡è®¾å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
            return file_uri

    def _parse_analysis_result(self, api_result: Dict[str, Any], video_path: str) -> Dict[str, Any]:
        """
        è§£æåˆ†æç»“æœï¼Œå‚è€ƒ video-analyzer çš„è§£ææ¨¡å¼

        Args:
            api_result: APIè¿”å›ç»“æœ
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            æ ¼å¼åŒ–çš„åˆ†æç»“æœ
        """
        try:
            # æå–æ–‡æœ¬å†…å®¹
            candidates = api_result.get('candidates', [])
            if not candidates:
                raise Exception("æ— æœ‰æ•ˆçš„åˆ†æç»“æœ")

            content = candidates[0].get('content', {})
            parts = content.get('parts', [])

            if not parts:
                raise Exception("åˆ†æç»“æœä¸ºç©º")

            analysis_text = parts[0].get('text', '')

            if not analysis_text:
                raise Exception("æœªè·å–åˆ°åˆ†ææ–‡æœ¬")

            logger.info(f"âœ… æˆåŠŸè·å–å“åº”æ–‡æœ¬ï¼Œé•¿åº¦: {len(analysis_text)}")

            # å°è¯•è§£æJSONæ ¼å¼çš„ç»“æœ
            analysis_data = None
            try:
                # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ‡è®°
                cleaned_text = analysis_text.strip()
                if cleaned_text.startswith('```json'):
                    cleaned_text = cleaned_text[7:]
                if cleaned_text.endswith('```'):
                    cleaned_text = cleaned_text[:-3]
                cleaned_text = cleaned_text.strip()

                if cleaned_text.startswith('{') or cleaned_text.startswith('['):
                    analysis_data = json.loads(cleaned_text)
                    logger.info("âœ… æˆåŠŸè§£æJSONæ ¼å¼çš„åˆ†æç»“æœ")
                else:
                    raise json.JSONDecodeError("Not JSON format", "", 0)

            except json.JSONDecodeError:
                # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æ ¼å¼
                logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬æ ¼å¼çš„åˆ†æç»“æœ")
                analysis_data = {
                    "content_analysis": {
                        "summary": analysis_text[:500] + "..." if len(analysis_text) > 500 else analysis_text,
                        "full_text": analysis_text
                    }
                }

            # æ„å»ºæ ‡å‡†åŒ–ç»“æœæ ¼å¼
            result = {
                "video_info": {
                    "file_name": Path(video_path).name,
                    "file_path": str(video_path),
                    "file_size": os.path.getsize(video_path),
                    "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "model_used": self.config.model_name,
                    "config": {
                        "cache_enabled": self.config.enable_cache,
                        "regions": self.config.regions
                    }
                },
                "analysis_result": analysis_data,
                "metadata": {
                    "response_length": len(analysis_text),
                    "candidates_count": len(candidates),
                    "success": True
                },
                "raw_response": api_result
            }

            return result

        except Exception as e:
            logger.error(f"âŒ ç»“æœè§£æå¤±è´¥: {e}")
            raise Exception(f"ç»“æœè§£æå¤±è´¥: {str(e)}")

    def clean_expired_cache(self) -> Dict[str, int]:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if not self.config.enable_cache:
            return {"removed": 0, "total": 0}

        try:
            cache_dir = Path(self.config.cache_dir)
            if not cache_dir.exists():
                return {"removed": 0, "total": 0}

            removed_count = 0
            total_count = 0
            current_time = time.time()

            for cache_file in cache_dir.glob("*.json"):
                total_count += 1
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)

                    if current_time - cache_data.get('timestamp', 0) > self.config.cache_expiry:
                        cache_file.unlink()
                        removed_count += 1

                except Exception:
                    # æŸåçš„ç¼“å­˜æ–‡ä»¶ï¼Œåˆ é™¤
                    cache_file.unlink()
                    removed_count += 1

            logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ: åˆ é™¤ {removed_count}/{total_count} ä¸ªæ–‡ä»¶")
            return {"removed": removed_count, "total": total_count}

        except Exception as e:
            logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
            return {"removed": 0, "total": 0}

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.enable_cache:
            return {"enabled": False}

        try:
            cache_dir = Path(self.config.cache_dir)
            if not cache_dir.exists():
                return {"enabled": True, "total_files": 0, "total_size": 0}

            total_files = 0
            total_size = 0
            oldest_timestamp = float('inf')
            newest_timestamp = 0

            for cache_file in cache_dir.glob("*.json"):
                total_files += 1
                total_size += cache_file.stat().st_size

                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    timestamp = cache_data.get('timestamp', 0)
                    oldest_timestamp = min(oldest_timestamp, timestamp)
                    newest_timestamp = max(newest_timestamp, timestamp)
                except Exception:
                    pass

            return {
                "enabled": True,
                "total_files": total_files,
                "total_size": total_size,
                "cache_dir": str(cache_dir),
                "oldest_entry": None if oldest_timestamp == float('inf') else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(oldest_timestamp)),
                "newest_entry": None if newest_timestamp == 0 else time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(newest_timestamp))
            }

        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {"enabled": True, "error": str(e)}


# ä¾¿åˆ©å‡½æ•°
def create_gemini_analyzer(
    cloudflare_project_id: str = "",
    cloudflare_gateway_id: str = "",
    google_project_id: str = "",
    regions: Optional[List[str]] = None,
    model_name: str = "gemini-2.5-flash",
    enable_cache: bool = True,
    cache_dir: str = ".cache/gemini_analysis"
) -> GeminiVideoAnalyzer:
    """
    åˆ›å»ºGeminiåˆ†æå™¨çš„ä¾¿åˆ©å‡½æ•°
    """
    config = GeminiConfig(
        cloudflare_project_id=cloudflare_project_id,
        cloudflare_gateway_id=cloudflare_gateway_id,
        google_project_id=google_project_id,
        regions=regions or ["us-central1", "us-east1", "europe-west1"],
        model_name=model_name,
        enable_cache=enable_cache,
        cache_dir=cache_dir
    )
    return GeminiVideoAnalyzer(config)


def test_gemini_analyzer():
    """æµ‹è¯•Geminiåˆ†æå™¨"""
    print("ğŸ§ª Geminiè§†é¢‘åˆ†æå™¨æµ‹è¯•")

    # ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºåˆ†æå™¨
    analyzer = create_gemini_analyzer(
        cloudflare_project_id="your_cloudflare_project_id",
        cloudflare_gateway_id="your_cloudflare_gateway_id",
        google_project_id="your_google_project_id"
    )

    # æµ‹è¯•è§†é¢‘è·¯å¾„
    test_video = "test_video.mp4"

    if not os.path.exists(test_video):
        print(f"âš ï¸  æµ‹è¯•è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_video}")
        print("è¯·å°†æµ‹è¯•è§†é¢‘æ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹test_videoå˜é‡")
        return

    # æµ‹è¯•æç¤ºè¯
    prompt = """è¯·åˆ†æè¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ–¹é¢ï¼š
1. åœºæ™¯æè¿°
2. ä¸»è¦ç‰©ä½“å’Œäººç‰©
3. åŠ¨ä½œå’Œæ´»åŠ¨
4. è§†é¢‘è´¨é‡è¯„ä¼°
5. æƒ…æ„Ÿè‰²è°ƒ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"""

    try:
        def progress_callback(progress: AnalysisProgress):
            print(f"è¿›åº¦: {progress.progress}% - {progress.step}")
            if progress.description:
                print(f"  æè¿°: {progress.description}")

        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        cache_stats = analyzer.get_cache_stats()
        print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {cache_stats}")

        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze_video(test_video, prompt, progress_callback)

        print("âœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ ç»“æœæ‘˜è¦:")
        print(f"  - æ–‡ä»¶: {result['video_info']['file_name']}")
        print(f"  - æ¨¡å‹: {result['video_info']['model_used']}")
        print(f"  - æ—¶é—´: {result['video_info']['analysis_time']}")
        print(f"  - å“åº”é•¿åº¦: {result['metadata']['response_length']}")

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file = f"analysis_result_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºæ›´æ–°åçš„ç¼“å­˜ç»Ÿè®¡
        cache_stats = analyzer.get_cache_stats()
        print(f"ğŸ“Š æ›´æ–°åç¼“å­˜ç»Ÿè®¡: {cache_stats}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_cache_functionality():
    """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ç¼“å­˜åŠŸèƒ½")

    analyzer = create_gemini_analyzer()

    # æ¸…ç†è¿‡æœŸç¼“å­˜
    clean_result = analyzer.clean_expired_cache()
    print(f"ğŸ§¹ ç¼“å­˜æ¸…ç†ç»“æœ: {clean_result}")

    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = analyzer.get_cache_stats()
    print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡: {stats}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "cache":
        test_cache_functionality()
    else:
        test_gemini_analyzer()
